load("~/Source-Code/mse231/project/data/tweet_with_hashtag.RData")
load("/Users/kaipingchen/Source-Code/mse231/project/data/tweet_with_hashtag.RData")
load("/Users/kaipingchen/Downloads/analyze_tweet_data_env.RData")
View(tweet_with_hashtag)
View(tweet_with_hashtag_with_prior)
View(tweet_with_hashtag)
load("/Users/kaipingchen/Downloads/analyze_tweet_data_env.RData")
View(tweet_with_hashtag_with_prior)
load("/Users/kaipingchen/Downloads/english_tweets_sentiment.RData")
head(tweets)
library(dplyr)
library(lubridate)
library(data.table)
install.packages("Lubridate")
install.packages("lubridate")
install.packages("data.table")
install.packages("tidyr")
install.packages("stringr")
install.packages("splitstackshape")
install.packages("bit64")
library(dplyr)
library(lubridate)
library(data.table)
library(tidyr)
library(stringr)
library(splitstackshape)
library(bit64)
library(tibble)
PreprocessTweetData <- function(tweet) {
# Create a date column
tweet <- tweet[, date := lubridate::date(datetime)]
# Find the daily volume by user and identify the users that are assumed to be bots (daily volume > 50)
dt.daily_volume <- tweet[, list(n = .N), by = c("user_id", "date")]
user_id_bots <- dt.daily_volume$user_id[dt.daily_volume$n > 50] # If the daily tweet volume > 50 for any of the days, identify the user as a bot
user_id_bots <- unique(user_id_bots)
# Include tweets that are belived to be generated by humans
tweet <- tweet[!(user_id %in% user_id_bots)]
# Change column types
colFactor = c("user_id", "language")
return(tweet)
}
tweet <- PreprocessTweetData(tweet)
tweet = load("/Users/kaipingchen/Downloads/english_tweets_sentiment.RData")
PreprocessTweetData <- function(tweet) {
# Create a date column
tweet <- tweet[, date := lubridate::date(datetime)]
# Find the daily volume by user and identify the users that are assumed to be bots (daily volume > 50)
dt.daily_volume <- tweet[, list(n = .N), by = c("user_id", "date")]
user_id_bots <- dt.daily_volume$user_id[dt.daily_volume$n > 50] # If the daily tweet volume > 50 for any of the days, identify the user as a bot
user_id_bots <- unique(user_id_bots)
# Include tweets that are belived to be generated by humans
tweet <- tweet[!(user_id %in% user_id_bots)]
# Change column types
colFactor = c("user_id", "language")
return(tweet)
}
tweet <- PreprocessTweetData(tweet)
tweets <- PreprocessTweetData(tweets)
AnalyzeHashtagsByTweet = function(tweet) {
tweet_with_hashtag <- tweet %>%
filter(hashtags != "") %>% #TODO: Omit this line?
mutate(hashtags = str_to_lower(hashtags),
hashtag_count = str_count(hashtags, " ") + 1)
tweet_with_hashtag <- tweet_with_hashtag %>%
rownames_to_column(var = "tweet_id") %>%
mutate(tweet_id = as.integer(tweet_id)) %>%
select(tweet_id, everything())
#cSplit("hashtags", sep = " ")
hashtags_pro_trump = c('altright', 'americafirst', 'clintonfoundation', 'crookedhillary', 'debatesideeffects', 'deplorable',
'gotrump', 'imwithyou', 'ivotetrump', 'latinosfortrump', 'lawandorder', 'maga', 'makeamericagrateagain',
'makeamericagreatagain', 'neverhillary', 'pepe', 'presidenttrump', 'realdonaldtrump', 'rnc', 'tcot',
'teamtrump', 'trumpforpresident', 'trumpiswithyou', 'trumppence16', 'trumppence2016', 'trumpsarmy',
'trumpstrong', 'trumptrain', 'trumpwins', 'trumpwon', 'votetrump', 'votetrump2016', 'wakeupamerica')
hashtags_pro_hillary = c('clintonkaine', 'clintonkaine16', 'clintonkaine2016', 'clintonwon', 'countrybeforeparty',
'ctl', 'dems', 'dirtydonald', 'dnc', 'factcheck', 'gohillary', 'hillaryforpresident',
'hillaryqualified', 'hillarysarmy', 'hillarywon', 'hillarywondebate', 'hillyes', 'iamwithher', 'imwither',
'imwithher', 'ivoteclinton', 'lovetrumpshate', 'momsdemandhillary', 'nevertrump', 'ohhillyes', 'p2', 'p2b',
'sheswithus', 'shewon', 'strongertogether', 'teamhillary', 'tntweeters', 'trumpeduptrickledown', 'uniteblue',
'votedems', 'votehillary', 'whyiwanthillary')
hashtags_anti_trump = c('antitrump', 'battrump', 'boycotttrump', 'chickentrump', 'debunkdonald', 'donalddeplorable',
'donaldtrumphandjokes', 'donaldtrumpthemovie', 'donaldtrumpwantstobanghisdaughter', 'donthecon',
'drumpf', 'dumbdonald', 'dumptrump', 'faketrumpintelligencebriefing', 'famousmelaniatrumpquotes',
'fuckdonaldtrump', 'fucktrump', 'literallytrump', 'loserdonald', 'lovetrumpshate', 'lyingtrump',
'makedonalddrumpfagain', 'momsagainsttrump', 'neverhillaryortrump', 'nevertrump', 'nevertrumporhillary',
'notrump', 'saferthanatrumprally', 'snifflingdonald', 'snifflingtrump', 'sociopathinchief', 'stopandfrisk',
'stopthetrumptrain', 'stoptrump', 'traitortrump', 'trump666', 'trumpdebateexcuses', 'trumpgirlsbreaktheinternet',
'trumpisracist', 'trumplies', 'trumplogic', 'trumplost', 'trumpocalypse', 'trumpolympics', 'trumpsacrifices',
'trumpsajoke', 'trumpsniff', 'trumpsniffle', 'trumpsniffles', 'trumpsoftballs', 'trumpsopoor', 'trumpsucks',
'trumptaxes', 'trumptrainwreck', 'trumpyourcat', 'weakdonald', 'whentrumpiselected', 'whitesagainsttrump')
hashtags_anti_hillary = c('basketofdeplorables', 'bathroomserver', 'benghazi', 'clintoncash', 'clintonemails',
'clintonnewsnetwork', 'corrupthillary', 'crookedhillary', 'hillaryforjail', 'hillaryforprison',
'hillaryforprison2016', 'hillaryhealth', 'hillarylies', 'hillarysemails', 'hillaryshealth',
'hitlary', 'imnotwithher', 'jillnothill', 'lyinghillary', 'neverhillary', 'neverhillaryortrump',
'nevertrumporhillary', 'notwithher', 'omittedhillaryquestions', 'paytoplay', 'shecanttellthetruth',
'sickhillary', 'whereshillary', 'whyimnotvotingforhillary')
tweet_with_hashtag.h <- tweet_with_hashtag %>%
cSplit("hashtags", sep = " ") %>%
tidyr::gather(hashtag_number, hashtag, hashtags_01:hashtags_23) %>%
dplyr::filter(!is.na(hashtag)) %>%
dplyr::select(tweet_id, hashtag) %>%
dplyr::mutate(is_pro_hillary = ifelse(hashtag %in% hashtags_pro_hillary, 1, 0),
is_pro_trump = ifelse(hashtag %in% hashtags_pro_trump, 1, 0),
is_anti_hillary = ifelse(hashtag %in% hashtags_anti_hillary, 1, 0),
is_anti_trump = ifelse(hashtag %in% hashtags_anti_trump, 1, 0))
tweet_with_hashtag.h.count <- tweet_with_hashtag.h %>%
group_by(tweet_id) %>%
summarise(cnt_pro_hillary = sum(is_pro_hillary),
cnt_pro_trump = sum(is_pro_trump),
cnt_anti_hillary = sum(is_anti_hillary),
cnt_anti_trump = sum(is_anti_trump))
tweet_with_hashtag.h.count <- as.data.table(tweet_with_hashtag.h.count)
tweet_with_hashtag.h.count[, cnt_political_hashtags := cnt_pro_hillary + cnt_pro_trump + cnt_anti_hillary + cnt_anti_trump]
tweet_with_hashtag.h.count[, tendency_pro_hillary := ifelse((cnt_pro_hillary + cnt_anti_hillary) > 0, (cnt_pro_hillary / (cnt_pro_hillary + cnt_anti_hillary)), 0)]
tweet_with_hashtag.h.count[, tendency_pro_trump := ifelse((cnt_pro_trump + cnt_anti_trump) > 0, (cnt_pro_trump / (cnt_pro_trump + cnt_anti_trump)), 0)]
tweet_with_hashtag.h.count[, tendency_anti_hillary := ifelse((cnt_pro_hillary + cnt_anti_hillary) > 0, (cnt_anti_hillary / (cnt_pro_hillary + cnt_anti_hillary)), 0)]
tweet_with_hashtag.h.count[, tendency_anti_trump := ifelse((cnt_pro_trump + cnt_anti_trump) > 0, (cnt_anti_trump / (cnt_pro_trump + cnt_anti_trump)), 0)]
tweet_with_hashtag <- as.data.table(tweet_with_hashtag)
tweet_with_hashtag <- merge(tweet_with_hashtag, tweet_with_hashtag.h.count, by = "tweet_id")
tweet_with_hashtag[, datetime := ymd_hms(datetime)]
return(tweet_with_hashtag)
}
tweet_with_hashtag <- AnalyzeHashtagsByTweet(tweet)
library(dplyr)
library(lubridate)
library(data.table)
library(tidyr)
library(stringr)
library(splitstackshape)
library(tibble)
library(bit64)
View(tweets)
load("/Users/kaipingchen/Downloads/english_tweets_sentiment.RData")
View(tweets)
PreprocessTweetData <- function(tweet) {
# Create a date column
tweet <- tweet[, date := lubridate::date(datetime)]
# Find the daily volume by user and identify the users that are assumed to be bots (daily volume > 50)
dt.daily_volume <- tweet[, list(n = .N), by = c("user_id", "date")]
user_id_bots <- dt.daily_volume$user_id[dt.daily_volume$n > 50] # If the daily tweet volume > 50 for any of the days, identify the user as a bot
user_id_bots <- unique(user_id_bots)
# Include tweets that are belived to be generated by humans
tweet <- tweet[!(user_id %in% user_id_bots)]
# Change column types
colFactor = c("user_id", "language")
return(tweet)
}
tweets <- PreprocessTweetData(tweets)
AnalyzeHashtagsByUser <- function(tweet) {
# Denote debate start and end times
debateStartTime = ymd_hms("2016-09-27 01:03:05 UTC") #fixed
debateEndTime = ymd_hms("2016-09-27 02:38:55 UTC") #fixed
# Identify users who tweeted both before and after the debate
valid_users <- tweet[, list(first_tweet = min(datetime, na.rm = TRUE),
last_tweet = max(datetime, na.rm = TRUE)), by = c("user_id")]
valid_users <- subset(valid_users, (first_tweet < debateStartTime & last_tweet > debateStartTime), select = user_id)
# Identify subgroup based on the pre-debate tweets
hashtagCountbyUser <- subset(tweet, (user_id %in% valid_users$user_id & datetime < debateStartTime))
hashtagCountbyUser <- hashtagCountbyUser[, list(preDebate_cnt_pro_hillary = sum(cnt_pro_hillary),
preDebate_cnt_pro_trump = sum(cnt_pro_trump),
preDebate_cnt_anti_hillary = sum(cnt_anti_hillary),
preDebate_cnt_anti_trump = sum(cnt_anti_trump),
preDebate_avg_sentiment = mean(sentiment, na.rm = TRUE),
preDebate_n_tweet = .N), by = c("user_id")]
hashtagCountbyUser[, preDebate_cnt_political_hashtags := preDebate_cnt_pro_hillary + preDebate_cnt_pro_trump + preDebate_cnt_anti_hillary + preDebate_cnt_anti_trump]
hashtagCountbyUser[, preDebate_tendency_pro_hillary := ifelse((preDebate_cnt_pro_hillary + preDebate_cnt_anti_hillary) > 0, (preDebate_cnt_pro_hillary / (preDebate_cnt_pro_hillary + preDebate_cnt_anti_hillary)), 0)]
hashtagCountbyUser[, preDebate_tendency_pro_trump := ifelse((preDebate_cnt_pro_trump + preDebate_cnt_anti_trump) > 0, (preDebate_cnt_pro_trump / (preDebate_cnt_pro_trump + preDebate_cnt_anti_trump)), 0)]
hashtagCountbyUser[, preDebate_tendency_anti_hillary := ifelse((preDebate_cnt_pro_hillary + preDebate_cnt_anti_hillary) > 0, (preDebate_cnt_anti_hillary / (preDebate_cnt_pro_hillary + preDebate_cnt_anti_hillary)), 0)]
hashtagCountbyUser[, preDebate_tendency_anti_trump := ifelse((preDebate_cnt_pro_trump + preDebate_cnt_anti_trump) > 0, (preDebate_cnt_anti_trump / (preDebate_cnt_pro_trump + preDebate_cnt_anti_trump)), 0)]
hashtagCountbyUser[, preDebate_group_pro_hillary := ifelse(preDebate_tendency_pro_hillary > 0.9, TRUE, FALSE)]
hashtagCountbyUser[, preDebate_group_pro_trump := ifelse(preDebate_tendency_pro_trump > 0.9, TRUE, FALSE)]
hashtagCountbyUser[, preDebate_group_anti_hillary := ifelse(preDebate_tendency_anti_hillary > 0.9, TRUE, FALSE)]
hashtagCountbyUser[, preDebate_group_anti_trump := ifelse(preDebate_tendency_anti_trump > 0.9, TRUE, FALSE)]
hashtagCountbyUser[, preDebate_group_neutral := ifelse((!preDebate_group_pro_hillary & !preDebate_group_pro_trump & !preDebate_group_anti_hillary & !preDebate_group_anti_trump) & abs(preDebate_avg_sentiment) < 0.001, TRUE, FALSE)]
hashtagCountbyUser[, hasPreDebateGroup := (preDebate_group_pro_hillary | preDebate_group_pro_trump | preDebate_group_anti_hillary | preDebate_group_anti_trump | preDebate_group_neutral)]
tweet_prior <- as.data.table(tweet)
tweet_prior <- merge(tweet_prior, hashtagCountbyUser, by = "user_id")
tweet_prior[, user_id := as.factor(user_id)]
# Add new columns
tweet_prior[, time := ifelse(datetime < debateStartTime, "before debate", ifelse(datetime <= debateEndTime, "during debate", "after debate"))]
tweet_with_prior[, time := as.factor(time)]
# Remove unused columns
tweet_prior[, c('language', 'number_mentions', 'date', 'hashtag_count') := NULL]
# Reorder columns
setcolorder(tweet_prior,
neworder = c('user_id', 'hasPreDebateGroup', 'preDebate_n_tweet', 'tweet_id', 'time', 'datetime', 'text', 'hashtags',
'sentiment', 'preDebate_avg_sentiment',
'group_pro_trump', 'group_pro_hillary', 'group_anti_trump', 'group_anti_hillary', 'group_neutral',
'cnt_pro_hillary', 'cnt_pro_trump', 'cnt_anti_hillary', 'cnt_anti_trump', 'cnt_political_hashtags',
'tendency_pro_hillary', 'tendency_pro_trump', 'tendency_anti_hillary', 'tendency_anti_trump',
'preDebate_cnt_pro_hillary', 'preDebate_cnt_pro_trump', 'preDebate_cnt_anti_hillary', 'preDebate_cnt_anti_trump', 'preDebate_cnt_political_hashtags',
'preDebate_tendency_pro_hillary', 'preDebate_tendency_pro_trump', 'preDebate_tendency_anti_hillary', 'preDebate_tendency_anti_trump',
'preDebate_group_pro_hillary', 'preDebate_group_pro_trump', 'preDebate_group_anti_hillary', 'preDebate_group_anti_trump', 'preDebate_group_neutral',
'urls', 'longitude', 'latitude', 'coordinates_type')
)
return(tweet_prior)
}
tweet_prior <- AnalyzeHashtagsByUser(tweets) # Note: this dataset includes only the users who tweeted (1) before AND (2) during or after the debate
AnalyzeHashtagsByTweet = function(tweet) {
tweet.h <- tweet %>%
cSplit("hashtags", sep = " ") %>%
tidyr::gather(hashtag_number, hashtag, hashtags_01:hashtags_23) %>%
dplyr::filter(!is.na(hashtag)) %>%
dplyr::select(tweet_id, hashtag) %>%
dplyr::mutate(is_pro_hillary = ifelse(hashtag %in% hashtags_pro_hillary, 1, 0),
is_pro_trump = ifelse(hashtag %in% hashtags_pro_trump, 1, 0),
is_anti_hillary = ifelse(hashtag %in% hashtags_anti_hillary, 1, 0),
is_anti_trump = ifelse(hashtag %in% hashtags_anti_trump, 1, 0))
tweet.h.count <- tweet.h %>%
group_by(tweet_id) %>%
summarise(cnt_pro_hillary = sum(is_pro_hillary),
cnt_pro_trump = sum(is_pro_trump),
cnt_anti_hillary = sum(is_anti_hillary),
cnt_anti_trump = sum(is_anti_trump))
tweet.h.count <- as.data.table(tweet.h.count)
tweet.h.count[, cnt_political_hashtags := cnt_pro_hillary + cnt_pro_trump + cnt_anti_hillary + cnt_anti_trump]
tweet.h.count[, tendency_pro_hillary := ifelse((cnt_pro_hillary + cnt_anti_hillary) > 0, (cnt_pro_hillary / (cnt_pro_hillary + cnt_anti_hillary)), 0)]
tweet.h.count[, tendency_pro_trump := ifelse((cnt_pro_trump + cnt_anti_trump) > 0, (cnt_pro_trump / (cnt_pro_trump + cnt_anti_trump)), 0)]
tweet.h.count[, tendency_anti_hillary := ifelse((cnt_pro_hillary + cnt_anti_hillary) > 0, (cnt_anti_hillary / (cnt_pro_hillary + cnt_anti_hillary)), 0)]
tweet.h.count[, tendency_anti_trump := ifelse((cnt_pro_trump + cnt_anti_trump) > 0, (cnt_anti_trump / (cnt_pro_trump + cnt_anti_trump)), 0)]
tweet <- as.data.table(tweet)
tweet <- merge(tweet, tweet.h.count, by = "tweet_id")
tweet[, datetime := ymd_hms(datetime)]
return(tweet)
}
tweets <- AnalyzeHashtagsByTweet(tweets)
a = list(1,2,3,4)
a
a=(1,2,3,4)
a=c(1,2,3,4)
a
min(a)
median(a)
a = c(1,2,3)
b=c(2,3,4)
c=c(4,5,6)
matrix (c(a,b,c),nrow=3, ncol=3))
matrix (c(a,b,c),nrow=3, ncol=3)
d=c(5,6,7)
matrix (c(a,b,c,d),nrow=4, ncol=3))
matrix (c(a,b,c,d),nrow=4, ncol=3)
matrix (c(a,b,c,d),nrow=3, ncol=4)
install.packages("glm2")
install.packages("glm.predict")
library(glm)
install.packages("glm")
library(glm2)
install.packages("leaps")
library(leaps)
table_type_call = table(call = resume$call, type = resume$type)
resume <- read.csv("resume.csv")
resume$type[resume$race == "black" & resume$sex == "female"] <- "BlackFemale"
resume$type[resume$race == "black" & resume$sex == "male"] <- "BlackMale"
setwd("~/Source-Code/com106-206-TA/KaipingAdina's Section Material/Lab2_DescriptiveStats")
resume <- read.csv("resume.csv")
resume$type[resume$race == "black" & resume$sex == "female"] <- "BlackFemale"
resume$type[resume$race == "black" & resume$sex == "male"] <- "BlackMale"
resume$type[resume$race == "white" & resume$sex == "female"] <- "WhiteFemale"
resume$type[resume$race == "white" & resume$sex == "male"] <- "WhiteMale"
resume$type <- as.factor(resume$type)
table_type_call = table(call = resume$call, type = resume$type)
table_type_call
resume <- read.csv("resume.csv")
resume$type[resume$race == "black" & resume$sex == "female"] <- "BlackFemale"
resume$type[resume$race == "black" & resume$sex == "male"] <- "BlackMale"
resume$type[resume$race == "white" & resume$sex == "female"] <- "WhiteFemale"
resume$type[resume$race == "white" & resume$sex == "male"] <- "WhiteMale"
resume$type <- as.factor(resume$type)
table(resume$type) # raw frequency count
table(resume$type, resume$call) ##table by category
table(type = resume$type, call = resume$call) ##table by category
prop.table(table(resume$type)) # proportion
table_type_call = table(call =resume$call, type = resume$type) ## Write the row variable first, then column variable
table_type_call
prop.table(table_type_call) ## this will provide percentage of each cell as part of the total population
prop.table(table_type_call, 1) # the "1" included in prop.table will tell R to make the rows add up to 100%
prop.table(table_type_call, 2) # the "2" included in prop.table will tell R to make columns add up to 100%
prop.table(table(call = resume$call, type = resume$type), 2) ## note prop.table function is for a table.
tapply(resume$call, resume$type, mean)
